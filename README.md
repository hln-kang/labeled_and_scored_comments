# labeled_and_scored_comments

I analyzed how Perspective API marked content that involved females versus males by comparing the returned toxicity scores. I hypothesized that Perspective would give comments that involve females a higher toxicity score than males. I found that Perspective did give comments involving females a higher toxicity score. Comments involving females had an average toxicity score of 0.598407, while comments involving males had an average of 0.573649. All pieces of data for female comments such as standard deviation, max/min, and quartiles had a greater toxicity score for female comments. I set my threshold to a score of 0.5 because it is half of the total score, and based on my data, what Perspective found as more toxic was usually above 0.5.

I think biases exist here because of human tendency to be biased when it comes to gender. These biases translate into the machine learning models we create. This makes me question machine learning models as it shows that they process data involving different genders yet the same content in different ways. Even though the comments were the exact same but used different pronouns, Perspective returned a higher toxicity score for females. 
